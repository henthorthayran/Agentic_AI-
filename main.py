# Semantic Kernel Tool Use Example

This document provides an overview and explanation of the code used to create a Semantic Kernel-based tool that integrates with ChromaDB for Retrieval-Augmented Generation (RAG). The example demonstrates how to build an AI agent that retrieves travel documents from a ChromaDB collection, augments user queries with semantic search results, and streams detailed travel recommendations.

## Initializing the Environment

SQLite Version Fix
If you encounter the error:
```
RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 >= 3.35.0
```

Uncomment this code block at the start of your notebook:

!pip install -r requirements.txt

%pip install pysqlite3
import sys
import pysqlite3
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

### Importing Packages
The following code imports the necessary packages:

import json
import os
import chromadb
from typing import Annotated, TYPE_CHECKING

from IPython.display import display, HTML

from openai import AsyncOpenAI

from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from semantic_kernel.contents import FunctionCallContent,FunctionResultContent, StreamingTextContent
from semantic_kernel.functions import kernel_function

if TYPE_CHECKING:
    from chromadb.api.models.Collection import Collection

### Creating the Semantic Kernel and AI Service

A Semantic Kernel instance is created and configured with an asynchronous OpenAI chat completion service. The service is added to the kernel for use in generating responses.

# Initialize the asynchronous OpenAI client
client = AsyncOpenAI(
    api_key="github_API",
    base_url="https://models.inference.ai.azure.com/"
)


# Create the OpenAI Chat Completion Service
chat_completion_service = OpenAIChatCompletion(
    ai_model_id="gpt-4o-mini",
    async_client=client,
)

### Defining the Prompt Plugin

The PromptPlugin is a native plugin that defines a function to build an augmented prompt using retrieval context

class GesturePromptPlugin:
    def __init__(self, collection: "Collection"):
        self.collection = collection

    @kernel_function(
        name="build_augmented_prompt",
        description="Create a prompt using gesture system context"
    )
    def build_augmented_prompt(self, query: str, retrieval_context: str) -> str:
        return (
            f"Retrieved Context:\n{retrieval_context}\n\n"
            f"User Query: {query}\n\n"
            "Answer strictly based on this context."
        )

    @kernel_function(name="retrieve_context", description="Search gesture setup info")
    def get_retrieval_context(self, query: str) -> str:
        results = self.collection.query(query_texts=[query], include=["documents", "metadatas"], n_results=3)
        entries = []
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            entries.append(f"Doc: {doc}\nMeta: {meta}")
        return "\n\n".join(entries) if entries else "No relevant setup info found."


### Defining Weather Information Plugin

The WeatherInfoPlugin is a native plugin that provides temperature information for specific travel destinations.

!pip install semantic_kernel



from typing import Annotated

class GestureCodePlugin:
    """A plugin to return or explain the full face gesture control setup code."""

    def __init__(self):
        # Store the full code as a long string
        self.full_code = """
import cv2
import time
import pyautogui
import mediapipe as mp
import numpy as np
from facenet_pytorch import InceptionResnetV1
import torch

# Screen size
screen_w, screen_h = pyautogui.size()

# Load FaceNet model for face recognition
# This model is pre-trained on the VGGFace2 dataset.
try:
    model = InceptionResnetV1(pretrained='vggface2').eval()
except Exception as e:
    print(f"Error loading FaceNet model: {e}")
    print("Please ensure you have an internet connection to download the pre-trained model.")
    exit()


# Load known face embeddings from saved .pt files
# These files should be generated by a separate script and contain the facial embeddings of authorized users.
try:
    known_faces = {
        "punit": torch.load("punit_face.pt"),
        "chinmay": torch.load("chinmay_face.pt"),
        "shachin": torch.load("shachin_face.pt"),
        "pushkar": torch.load("pushkar_face.pt"),
    }
except FileNotFoundError as e:
    print(f"Error loading face embedding file: {e}")
    print("Please make sure the .pt files for known faces are in the same directory as the script.")
    exit()


# MediaPipe Setup for Hand and Face Detection
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5)
mp_draw = mp.solutions.drawing_utils
mp_face_detection = mp.solutions.face_detection
face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.6)

# --- STATE & CONSTANTS ---

# State Flags to manage actions and cooldowns
face_detected = False
last_zoom_time = 0
last_volume_time = 0
last_scroll_time = 0
last_mouse_move = 0
double_click_start = None

# Disable the failsafe to prevent the script from stopping unexpectedly.
# Be cautious: this makes it harder to stop the script if it misbehaves.
pyautogui.FAILSAFE = False

# Cooldown periods (in seconds) to prevent actions from firing too rapidly
ZOOM_COOLDOWN = .8
VOLUME_COOLDOWN = .5
SCROLL_COOLDOWN = .1
MOUSE_COOLDOWN = 0.01
DOUBLE_CLICK_DURATION = 1

# Initialize mouse position
last_index_pos = None
mouse_mode_active = False
MOUSE_SENSITIVITY = 3


# Function to get the embedding of a face image
def get_face_embedding(face_img):
    # Resize image to the model's required input size (160x160)
    face_img = cv2.resize(face_img, (160, 160))
    # Preprocess the image: transpose dimensions, normalize, and convert to a tensor
    face_img = np.transpose(face_img, (2, 0, 1)) / 255.0
    face_tensor = torch.tensor(face_img, dtype=torch.float32).unsqueeze(0)
    # Get the embedding from the model
    return model(face_tensor).detach()

def fingers_up(lmList):
    tip_ids = [4, 8, 12, 16, 20]
    finger_states = []
    for i in range(1, 5):
        finger_states.append(lmList[tip_ids[i]][1] < lmList[tip_ids[i] - 2][1])
    thumb_up = lmList[tip_ids[0]][0] > lmList[tip_ids[0] - 1][0]
    return [thumb_up] + finger_states

def get_distance(p1, p2):
    return np.linalg.norm(np.array(p1) - np.array(p2))

def get_center(p1, p2):
    return tuple(np.mean([p1, p2], axis=0).astype(int))

# Camera Setup
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    h, w, _ = frame.shape
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    current_time = time.time()


    # --- Face Recognition (only once) ---
    if not face_detected:
        result_face = face_detection.process(rgb)
        if result_face.detections:
            for detection in result_face.detections:
                bboxC = detection.location_data.relative_bounding_box
                x1 = max(0, int(bboxC.xmin * w))
                y1 = max(0, int(bboxC.ymin * h))
                x2 = min(w, int(x1 + bboxC.width * w))
                y2 = min(h, int(y1 + bboxC.height * h))
                face_img = frame[y1:y2, x1:x2]

                if face_img.size == 0:
                    continue

                # Get the embedding for the detected face
                embedding = get_face_embedding(face_img)
                min_dist = float('inf')
                identity = "Unknown"

                # Compare with known faces
                for name, known_embedding in known_faces.items():
                    dist = torch.dist(embedding, known_embedding).item()
                    # If distance is below a threshold (0.8), we have a potential match
                    if dist < min_dist and dist < 0.8:
                        min_dist = dist
                        identity = name

                # Draw bounding box and name on the frame
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)
                cv2.putText(frame, identity, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

                if identity == "punit":
                    face_detected = True
                    print("✅ Punit recognized — gesture control enabled")
    if face_detected:
        # --- Gesture Control ---
        result_hands = hands.process(rgb)
        if result_hands.multi_hand_landmarks:
            for handLms in result_hands.multi_hand_landmarks:
                # Draw hand landmarks
                lmList = [(int(lm.x * w), int(lm.y * h)) for lm in handLms.landmark]
                mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)

                if lmList:
                    fingers = fingers_up(lmList)

                    # --- GESTURE TO ACTION MAPPING ---

                    # Fist
                    if fingers == [False, False, False, False, False]:
                                            pass
                    # ✋ All fingers up → Mouse Pad mode
                    if fingers == [True, True, True, True, True]:
                        index_tip = lmList[8]

                        if not mouse_mode_active:
                            # First frame after mouse mode activates — don't move yet
                            last_index_pos = index_tip
                            mouse_mode_active = True
                        else:
                            if last_index_pos is not None and current_time - last_mouse_move > MOUSE_COOLDOWN:
                                dx = (index_tip[0] - last_index_pos[0]) * MOUSE_SENSITIVITY
                                dy = (index_tip[1] - last_index_pos[1]) * MOUSE_SENSITIVITY

                                pyautogui.moveRel(-dx, dy)  # Invert x for mirror effect
                                last_mouse_move = current_time

                            last_index_pos = index_tip
                    else:
                        mouse_mode_active = False  # Deactivate mouse mode but don't reset last_index_pos yet

                    # Zoom
                    if fingers[1] and fingers[4] and not fingers[2] and not fingers[3]:
                        dist = get_distance(lmList[4], lmList[8])
                        if current_time - last_zoom_time > ZOOM_COOLDOWN:
                            if dist < 40:
                                print("Zoom Out")
                                pyautogui.hotkey('ctrl', '-')
                            elif dist > 90:
                                print("Zoom In")
                                pyautogui.hotkey('ctrl', '=')
                            last_zoom_time = current_time

                    # Volume Control
                    if fingers == [True, True, False, False, False]:
                        if current_time - last_volume_time > VOLUME_COOLDOWN:
                            if lmList[4][1] < lmList[2][1]:
                                print("Volume Up")
                                pyautogui.press("volumeup")
                            else:
                                print("Volume Down")
                                pyautogui.press("volumedown")
                            last_volume_time = current_time

                    # Scroll Control
                    if fingers == [False, True, True, False, False]:
                        index_tip_y = lmList[8][1]
                        if current_time - last_scroll_time > SCROLL_COOLDOWN:
                            if index_tip_y <= 80:
                                print("Scroll Up")
                                pyautogui.scroll(50)
                            elif index_tip_y >= 280:
                                print("Scroll Down")
                                pyautogui.scroll(-50)
                            last_scroll_time = current_time

                    # Double Click
                    if fingers == [False, True, True, True, False]:
                        if double_click_start is None:
                            double_click_start = current_time
                        elif current_time - double_click_start >= DOUBLE_CLICK_DURATION:
                            print("Double Click")
                            pyautogui.doubleClick()
                            double_click_start = None
                    else:
                       # If the gesture is no longer held, reset the double click timer
                        double_click_start = None
        else:
            # If no hands are detected, reset the double click timer
            double_click_start = None

        cv2.imshow("Face + Gesture Control", frame)
        if cv2.waitKey(1) & 0xFF == 27:
            break

cap.release()
cv2.destroyAllWindows()

"""  # Replace this with your full actual code

    @kernel_function(
        name="get_full_code",
        description="Returns the segement of face gesture control setup code that part i will ask."
    )
    def get_full_code(self) -> Annotated[str, "The entire face gesture control Python code."]:
        return self.full_code


### Defining Destinations Information Plugin

The DestinationsPlugin is a native plugin that provides detailed information about popular travel destinations.

from semantic_kernel.functions import kernel_function

class LibraryInfoPlugin:
    """Provides information about the libraries used in the face gesture control setup."""

    LIBRARY_INFO = {
        "opencv": {
            "name": "OpenCV",
            "description": "A computer vision library used for real-time image and video processing.",
            "used_for": ["Capturing webcam feed", "Displaying frames", "Drawing bounding boxes or landmarks"],
            "official_site": "https://opencv.org/"
        },
        "mediapipe": {
            "name": "MediaPipe",
            "description": "A Google open-source framework for building multimodal ML pipelines.",
            "used_for": ["Hand tracking", "Pose estimation", "Face detection"],
            "official_site": "https://mediapipe.dev/"
        },
        "pyautogui": {
            "name": "PyAutoGUI",
            "description": "A Python library for GUI automation, allowing mouse and keyboard control.",
            "used_for": ["Mouse movement", "Clicking", "Scrolling", "Keyboard simulation"],
            "official_site": "https://pyautogui.readthedocs.io/"
        },
        "numpy": {
            "name": "NumPy",
            "description": "A fundamental package for scientific computing with Python.",
            "used_for": ["Array manipulations", "Mathematical operations", "Coordinate calculations"],
            "official_site": "https://numpy.org/"
        },
        "torch": {
            "name": "PyTorch",
            "description": "An open-source machine learning framework for deep learning applications.",
            "used_for": ["Running face recognition models", "Handling tensors and GPU inference"],
            "official_site": "https://pytorch.org/"
        },
        "facenet_pytorch": {
            "name": "FaceNet (PyTorch)",
            "description": "A PyTorch implementation of the FaceNet model for face embeddings.",
            "used_for": ["Generating face embeddings", "Face recognition by comparing vectors"],
            "official_site": "https://github.com/timesler/facenet-pytorch"
        }
    }

    @kernel_function(
        name="get_library_info",
        description="Provides detailed information about libraries used in face gesture control system."
    )
    def get_library_info(self, query: str) -> str:
        query_lower = query.lower()
        matches = []

        for key, lib in self.LIBRARY_INFO.items():
            if key in query_lower or lib["name"].lower() in query_lower:
                matches.append(lib)

        if not matches:
            return (
                f"User Query: {query}\n\n"
                "No matching library found in the project context. "
                "Ensure you're referring to one of the project’s used libraries."
            )

        result = "\n\n".join([
            f"Library: {lib['name']}\n"
            f"Description: {lib['description']}\n"
            f"Used For: {', '.join(lib['used_for'])}\n"
            f"Official Site: {lib['official_site']}"
            for lib in matches
        ])

        return (
            f"Library Information:\n{result}\n\n"
            f"User Query: {query}\n\n"
            "Based on the above, answer the user's question accurately."
        )

from semantic_kernel.functions import kernel_function

class GestureActionPlugin:
    """Provides detailed information about hand gestures and the actions they perform."""

    GESTURE_ACTIONS = {
        "zoom": {
            "gesture": "Thumb and index finger tip distance with pinkey finger up",
            "action": "Zoom in/out",
            "how_it_works": (
                "The Euclidean distance between the thumb tip and index tip is calculated. "
                "If the pinkey finger is up, zoom is active. "
                "The system compares the current distance to a reference to determine zoom scale."
            )
        },
        "mouse_control": {
            "gesture": "All five fingers up (open palm)",
            "action": "Control mouse cursor",
            "how_it_works": (
                "When all five fingers are detected as up, the index fingertip coordinates are used "
                "to move the cursor using PyAutoGUI. Smoothening is applied to reduce jitter."
            )
        },
        "volume_up": {
            "gesture": "Thumb up and index finger up",
            "action": "Increase system volume",
            "how_it_works": (
                "When the thumb and index finges up and other fingers are down, volume is increased gradually using a system command."
            )
        },
        "volume_down": {
            "gesture": "Thumb downward up and index finger up",
            "action": "Decrease system volume",
            "how_it_works": (
                "When the thumb is pointing downward and index up and other fingers are down, volume is decreased using automation."
            )
        },
        "scroll": {
            "gesture": "Index and middle finger up inside screen region",
            "action": "Scroll vertically",
            "how_it_works": (
                "When both index and middle fingers are up and their combined position lies in a defined screen region like upward region then scroll up and if downward region then scroll down "
                "scroll up/down is triggered based on vertical motion using PyAutoGUI."
            )
        },
        "double_click": {
            "gesture": "Index, middle, and ring fingers up for 1 seconds",
            "action": "Double-click at mouse position",
            "how_it_works": (
                "When three fingers (index, middle, ring) are up continuously for 1+ seconds, a double-click is triggered using PyAutoGUI."
            )
        },

    }

    @kernel_function(
        name="get_gesture_action_info",
        description="Provides detailed information about hand gestures and their corresponding actions."
    )
    def get_gesture_action_info(self, query: str) -> str:
        query_lower = query.lower()
        matches = []

        for key, data in self.GESTURE_ACTIONS.items():
            if key in query_lower or data["action"].lower() in query_lower:
                matches.append(data)

        if not matches:
            return (
                f"User Query: {query}\n\n"
                "I couldn't find matching gesture-action information. "
                "Please ask about gestures like zoom, mouse control, scroll, volume, double-click, or app launch."
            )

        result = "\n\n".join([
            f"Gesture: {data['gesture']}\n"
            f"Action: {data['action']}\n"
            f"How it Works: {data['how_it_works']}"
            for data in matches
        ])

        return (
            f"Gesture Action Information:\n{result}\n\n"
            f"User Query: {query}\n\n"
            "Based on the above gesture definition, provide a relevant answer."
        )

## Setting Up ChromaDB

To facilitate Retrieval-Augmented Generation, a persistent ChromaDB client is instantiated and a collection named `"travel_documents"` is created (or retrieved if it exists). This collection is then populated with sample travel documents and metadata.

collection = chromadb.PersistentClient(path="./chroma_db").create_collection(
    name="gesture_documents",
    metadata={"description": "Hand gesture control system documentation"},
    get_or_create=True,
)

documents = [

    # Architecture Layering
    "System Architecture: The gesture control system is composed of three core layers: "
    "1) Input Layer (camera + MediaPipe Hand landmarks), "
    "2) Gesture Logic Layer (finger state inference, gesture detection), "
    "3) Action Layer (system commands via PyAutoGUI or OS-level hooks). "
    "Each layer is modular and testable in isolation.",

    # Gesture Conflict Resolution
    "Gesture Conflict Handling: When multiple gestures match simultaneously (e.g., all fingers up AND thumb up), "
    "the system resolves priority based on gesture specificity. For example, volume control (thumb-only) "
    "takes precedence over general mouse mode. A gesture priority table is maintained to avoid ambiguity.",

    # Gesture State Management
    "Gesture State Machine: The system maintains current, previous, and cooldown state for each gesture. "
    "Transitions only occur if the gesture is held for a minimum time (gesture hold time). "
    "Cooldown prevents retriggering within defined timeout periods. State persistence helps in fluid control.",

    # Ambient Light & Detection Reliability
    "Detection Robustness: Ambient lighting, skin tone, and hand orientation can affect landmark reliability. "
    "The system includes heuristics for confidence filtering (based on MediaPipe's hand detection score) "
    "and ignores hands with <80% confidence. Optionally, light sensors or face brightness can modulate thresholds.",

    # Multi-User Handling
    "Multiple User Support: For shared environments, the system tags active gestures to the closest detected face "
    "if combined with facial recognition. Gesture ownership is bound to face embeddings (via ArcFace or FaceNet). "
    "This prevents command theft or misfiring in group scenarios.",

    # Gesture Timeout Safety
    "Safety Timeout: If no valid gesture is detected for 10 seconds, the system enters idle mode. "
    "This avoids accidental triggers when the user steps away. Idle mode disables control and awaits a reactivation gesture.",

    # Error Recovery & Fallbacks
    "Error Handling: If MediaPipe fails to detect a hand or gesture breaks mid-action, the system gracefully exits the gesture mode "
    "and provides a visual or audio cue. For persistent failures, it logs errors and optionally restarts the input stream.",

    # Add-On: Facial Expression Trigger
    "Facial Triggers: Advanced extensions allow blending face expressions like eyebrow raise or smile (using face landmarks) "
    "to activate special modes. For example, a smile while showing a fist could launch a 'fun mode'.",

    # Voice-Aided Confirmation
    "Voice Feedback Layer: After any system action, a voice module (like pyttsx3) speaks the performed action "
    "such as 'Scrolling down', 'Mouse activated'. This aids accessibility and error awareness. Can be disabled in settings.",

    # Performance & FPS Tuning
    "Performance Optimization: Gesture detection runs asynchronously at 15–30 FPS, depending on CPU/GPU. "
    "The system downsamples the frame size to 640x480 for optimal detection. Only every 3rd frame may be used for gesture logic, "
    "and smoothing filters reduce flickering in mouse control."


    '''📁 Document Title: Face & Hand Gesture Control System Overview

📌 Objective:
To build a secure and intuitive gesture-based control interface using face recognition for authentication and hand gestures for device control (e.g., mouse, volume, zoom, scrolling, etc.).

🧠 Tech Stack:

OpenCV: Video capture and image processing

MediaPipe: Real-time face and hand landmark detection

FaceNet (via facenet-pytorch): Deep learning face recognition

PyAutoGUI: Simulates keyboard/mouse actions

NumPy & Torch: Numerical and ML backend support


📂 System Workflow:

Authentication – Face Recognition:

Uses FaceNet (InceptionResnetV1) model pretrained on VGGFace2.

Embeddings from authorized users (.pt files) are compared using cosine distance.

Gesture control is only activated once a known face (e.g., "punit") is recognized.

Hand Detection – MediaPipe Hands:

Detects one hand and tracks 21 landmarks.

Converts landmark positions to pixel coordinates for gesture mapping.

Gesture → Action Mapping:

Gesture Pattern	Action	Notes
[True, True, True, True, True]	Mouse control	All fingers up — moves the mouse relative to index finger movement.
[True, True, False, False, False]	Volume up/down	Based on thumb position vs wrist.
[False, True, True, False, False]	Scroll up/down	Based on Y-position of index fingertip.
[False, True, True, True, False]	Double click	Triggered if held for ≥ 1 second.
[True, False, False, False, True]	Zoom in/out	Based on distance between thumb and index finger.
[False, False, False, False, False]	Idle/Fist	No action.

State & Cooldowns:

face_detected: Triggered once identity is confirmed.

Cooldown timers (ZOOM_COOLDOWN, SCROLL_COOLDOWN, etc.) prevent rapid-fire actions.


🔐 Security & Safety Features:

Failsafe Disabled: Allows full mouse control, but use with caution.

Face Gating: Only authorized users can access gesture control.

Cooldown Timers: Prevent accidental repeated actions.


🎯 Use Case:
This system can be used for:

Hands-free desktop interaction

Accessibility interfaces

Smart home controls

Human-computer interaction research

✅ Summary:
The system offers a secure, vision-based control mechanism by combining face authentication with dynamic gesture interpretation. Once authenticated, gestures trigger real-time OS-level actions like scrolling, zooming, or volume changes—mimicking mouse and keyboard behavior without touch.'''

    '''This project is a sophisticated real-time Human-Computer Interaction (HCI) system that leverages advanced machine learning models for biometric authentication and gesture-based computer control. Its architecture is divided into two distinct, sequential phases: Authentication and Control.

Phase 1: Biometric Authentication via Deep Metric Learning (FaceNet)
This phase acts as a security gate, ensuring that only an authorized individual can operate the system. It goes far beyond simple template matching by using a deep learning technique called metric learning.

The Core Concept: Face Embeddings
The fundamental challenge in face recognition is that a person's appearance varies significantly with lighting, angle, and expression. The script solves this by not comparing images directly, but by comparing abstract mathematical representations of them, known as embeddings.

Model Architecture: The script uses InceptionResnetV1, a powerful Deep Convolutional Neural Network (CNN). This model has been pre-trained on the massive VGGFace2 dataset, which contains millions of face images from thousands of individuals.

Triplet Loss Training: The model was trained using a technique called Triplet Loss. Instead of just classifying images ("this is Punit"), it learns to create a high-dimensional "face space." In this space:

Embeddings of different images of the same person are mathematically close together.

Embeddings of images of different people are pushed far apart.

Inference in the Script:

The get_face_embedding function performs an inference pass. It takes a detected face, processes it into the required format (a 160x160 tensor), and feeds it through the trained FaceNet model.

The output is a 512-dimensional vector (the embedding) that serves as a unique, numerical signature for that face.

The script then calculates the Euclidean distance (L
2
​
  norm) between the embedding of the face in the camera and the pre-computed embeddings of known users (torch.dist). A small distance (e.g., < 0.8) signifies a high probability of a match.

This approach is robust because the model has learned the essential, invariant features of a face, making it resilient to minor changes in appearance.

Phase 2: Gesture Control via Real-Time Landmark Tracking (MediaPipe)
Once authenticated, the system transitions to the control phase, which relies on Google's MediaPipe framework for its exceptional speed and accuracy.

The Two-Stage Pipeline
Tracking a hand and all its joints across every pixel of a high-resolution video frame in real-time is computationally intensive. MediaPipe solves this with a highly optimized, two-stage pipeline:

Palm Detector: First, a very lightweight, fast detector runs on the entire frame. Its only job is to find the approximate position and orientation of a hand's palm.

Hand Landmark Model: Once the palm is located, a more complex and precise model is run only on that small, cropped region of interest. This model regresses the 21 key 3D coordinates (x,y,z) of the hand's joints and fingertips.

This architecture dramatically reduces the computational load, allowing for smooth, real-time performance even without a high-end GPU.

From Landmarks to Actions
Gesture Vocabulary: The script defines a "vocabulary" of gestures in the fingers_up function. By comparing the y-coordinates of the fingertip landmarks to the y-coordinates of the lower knuckle landmarks, it creates a boolean list representing the state of each finger (e.g., [False, True, True, False, False] for a "peace sign").

Stateful Interaction: The system is not merely reactive; it's stateful. This is critical for creating a usable interface.

Mode Switching: The mouse_mode_active flag ensures that the system explicitly enters and exits mouse control mode, preventing unintended cursor movement.

Sustained Gestures: The double_click_start timer requires the user to hold a gesture for a specific duration (DOUBLE_CLICK_DURATION). This design pattern prevents an action from being triggered accidentally while transitioning between gestures.

Debouncing: The *_COOLDOWN constants act as a "debounce" mechanism, preventing a single, continuous gesture from firing an action hundreds of times per second (e.g., preventing volume from shooting up instantly).

System Integration and Limitations
Architecture: The entire system is a simple but effective state machine, governed by the face_detected boolean. It moves from an "unauthenticated" state to an "authenticated and active" state.

HCI Loop: The script creates a tight Human-Computer Interaction loop. The user performs a gesture, the system interprets it, PyAutoGUI executes the command, and the user sees the result on their screen while also seeing the landmark overlay on the cv2 window, providing immediate visual feedback.

Limitations:

Environmental Dependency: Performance is highly dependent on good, consistent lighting.

Hardcoded Logic: The authorized user ('punit') and the gesture mappings are hardcoded. A more scalable application would use a configuration file or a database.

Security Risk: The use of pyautogui.FAILSAFE = False is a significant security and usability risk, as it removes the primary mechanism for stopping a runaway script.'''
  ''' Practical Applications and Use Cases
This project, while a prototype, demonstrates a powerful interaction paradigm with real-world applications:

Accessibility ♿: This is the most significant use case. Individuals with motor impairments or physical disabilities who cannot use a traditional mouse and keyboard could use this system to interact with a computer, enhancing their independence.

Sterile Environments 👨‍⚕️: In operating rooms, scientific clean rooms, or industrial labs, professionals need to interact with computers without touching surfaces. Gesture control allows surgeons to review medical images or technicians to operate machinery without compromising sterility.

Presentations and Public Kiosks 🎤: A presenter could control their slideshow with simple hand gestures, eliminating the need for a physical clicker. Similarly, interactive displays in museums or lobbies could be controlled without physical contact, improving hygiene.

Creative and Industrial Design 🎨: Artists or designers using CAD software could use one hand on a keyboard for shortcuts while using the other hand for intuitive, gesture-based manipulation of 3D models.

Code Improvements and Extensibility
To move this from a prototype to a robust application, several improvements could be made:

Dynamic User Enrollment: Instead of hardcoding authorized users ("punit": torch.load(...)), you could create a separate "enrollment" script. This script would:

Ask for a new user's name.

Capture several images of their face from the webcam.

Generate the average embedding from these images.

Save the embedding to a file named after the user (e.g., user_embeddings/john_doe.pt).
The main script would then load all .pt files from that directory at startup.

Configurable Gesture Engine: The current if/elif chain for gestures is rigid. A more flexible approach would be to use a dictionary to map gestures to functions.

Python

# Example of a more flexible gesture mapping
def perform_scroll_up():
    pyautogui.scroll(50)

def perform_double_click():
    pyautogui.doubleClick()

# Gesture representation (e.g., a tuple of booleans) -> function to call
gesture_actions = {
    (False, True, True, False, False): perform_scroll_up,
    (False, True, True, True, False): perform_double_click
    # Add other gestures and functions here
}

# In the main loop:
fingers_tuple = tuple(fingers_up(lmList))
if fingers_tuple in gesture_actions:
    gesture_actions[fingers_tuple]() # Calls the appropriate function
Graphical User Interface (GUI): Using a library like Tkinter or PyQt, you could build a settings panel. This would allow the user to:

Adjust mouse sensitivity and cooldown timers.

Enable or disable specific gestures.

Receive visual feedback (e.g., an icon showing the currently active gesture) outside of the camera window.

Performance and Ethical Considerations
Hardware and Performance:

CPU-Bound: MediaPipe is highly optimized to run on standard CPUs. While a GPU would help, the system is designed to be accessible without specialized hardware.

Camera is Key: The system's reliability is directly tied to the camera's quality. A higher frame rate (FPS) and better performance in low light will lead to smoother, more accurate tracking.

Lag (Latency): There is a small, unavoidable delay between performing a gesture and the computer's response. This is due to the time taken for frame capture, model inference (FaceNet + MediaPipe), and command execution. On older machines, this lag might be more noticeable.

Ethical Considerations and Bias:

Facial Recognition Bias: All facial recognition models carry an inherent risk of bias. A model trained on a dataset that underrepresents certain demographic groups may have a higher error rate for individuals from those groups. This could lead to authorized users being locked out of the system.

Privacy: This system requires a camera to be constantly active, observing the user and their environment. In a real product, strict privacy controls and transparency about when the system is active and what data is being processed are non-negotiable.

Security: Facial recognition is not foolproof. It can potentially be tricked by high-resolution photos or videos, a technique known as "spoofing." For applications requiring high security, it should be used as one part of a multi-factor authentication system, not as the sole authenticator.'''
 '''Document Title: Secure Face-Activated Hand Gesture Control System

1. Overview

This project is an advanced human-computer interaction (HCI) system that integrates real-time facial recognition with dynamic hand gesture tracking to enable secure, touchless control of computer functionalities such as mouse movement, zooming, scrolling, volume adjustment, and double-clicking. This system is highly suitable for environments that demand hygienic, accessible, or hands-free operation.

2. Components and Technologies

OpenCV: Used for accessing the webcam and performing image processing operations.

MediaPipe:

Face Detection: Identifies and localizes faces within the camera frame.

Hand Tracking: Detects and tracks hand landmarks in real-time.

FaceNet (facenet-pytorch): A deep learning model used to generate embeddings for face recognition.

PyAutoGUI: Allows programmatic control of the mouse and keyboard.

Torch (PyTorch): Performs tensor operations and deep learning inference.

NumPy: Facilitates numerical calculations.

3. Face Recognition Phase

On initialization, the system waits for a face to be detected.

Once a face is detected by MediaPipe, it extracts the bounding box and crops the face region.

The image is resized to 160x160 and converted into a normalized tensor.

FaceNet generates a 128-dimensional embedding for the detected face.

The system compares this embedding with pre-saved embeddings (.pt files) of authorized users.

If the distance between embeddings is below a threshold (0.8), the face is considered recognized.

Only after face authentication is successful, gesture control is enabled for that session.

4. Hand Gesture Recognition Phase

Once a face is authenticated, MediaPipe continuously tracks the right hand.

It identifies 21 key hand landmarks and provides coordinates for each.

These landmarks are used to interpret finger states and recognize gestures.

5. Gesture to Action Mappings

Gesture Description

Finger Pattern

Action Triggered

Cooldown (seconds)

Fist (Idle)

All fingers down

No Action

-

Mouse Pad Mode

All fingers up

Move mouse with index tip

0.01

Zoom In/Out

Thumb + Index up only

Ctrl + / Ctrl -

0.8

Volume Control

Thumb + Index up

Volume Up/Down

0.5

Scroll Control

Index + Middle up

Scroll Up/Down

0.1

Double Click

Index + Middle + Ring up

Mouse double-click

Hold for 1 sec

6. Mouse Movement Control

Mouse control is activated when all fingers are up.

The system tracks the index fingertip movement relative to the last frame.

This delta is scaled by a MOUSE_SENSITIVITY factor (3x).

X-axis is mirrored to simulate natural cursor movement.

7. Zoom Control Logic

Activated when only the Thumb and Index fingers are up.

Measures Euclidean distance between tip of Thumb (id 4) and Index (id 8).

If distance < 40: Triggers "Zoom Out" (Ctrl -).

If distance > 90: Triggers "Zoom In" (Ctrl +).

Debounced using ZOOM_COOLDOWN.

8. Volume Control Logic

Triggered when Thumb and Index are up.

If Thumb tip (y) is above the knuckle, it simulates volumeup.

If below, it simulates volumedown.

9. Scroll Control Logic

Activated when Index and Middle fingers are up.

Uses the y-position of the index finger:

y < 80 px: Scroll up

y > 280 px: Scroll down

Controlled using SCROLL_COOLDOWN.

10. Double Click Gesture

Requires Index, Middle, and Ring fingers to be up.

Triggers double-click if held continuously for 1 second.

Automatically resets if gesture breaks.

11. Safeguards and States

pyautogui.FAILSAFE = False: Prevents automatic exit if mouse hits screen corner.

System only enters gesture mode if face_detected = True.

Cooldowns are managed per action to prevent rapid firing.

double_click_start tracks gesture hold duration.

12. Application Use Cases

Accessibility: Allows users with physical impairments to control computers hands-free.

Smart Offices/Classrooms: Navigate content during presentations without touching any device.

Sanitized Environments: Hospitals, labs where hands-free operation is critical.

Public Interfaces: Kiosks or ATMs with face-authenticated touchless interaction.

13. System Limitations

Performance depends on lighting and camera quality.

Only supports one user and one hand at a time.

Not yet optimized for GPU acceleration or multi-threading.

Works only when authorized face is detected once at start.

14. Summary

This is a secure, modular and intuitive HCI system that combines computer vision, deep learning, and gesture control into a unified interface. It enables verified users to interact with a system using natural gestures, ensuring both convenience and security. It is ideal for building intelligent, contactless, and user-friendly environments.'''


]


# Add documents to the vector store
collection.add(
    documents=documents,
    ids=[f"gesture_doc_{i}" for i in range(len(documents))],
    metadatas=[{"source": "gesture_doc", "type": "gesture_info"} for _ in documents]
)

agent = ChatCompletionAgent(
    service=chat_completion_service,
    plugins=[
        GesturePromptPlugin(collection),
        GestureCodePlugin(),
        GestureActionPlugin(),
        LibraryInfoPlugin(),
    ],
    name="GestureControlAgent",
    instructions=(
        "give all answer in politely form"
        "You are an expert assistant that helps the user understand, debug, and extend their "
        "gesture-based control system using MediaPipe, FaceNet, and PyAutoGUI. "
        "Use plugin context when available. Prioritize gesture mapping, architecture, performance, or integration questions. "
        "If a question is about code, explain and optionally provide working code examples. "
        "If no matching context is found, clarify that you’re extrapolating from known system behavior."
    )
)

### Running the Agent with Streaming Chat History
The main asynchronous loop creates a chat history for the conversation and, for each user input, first adds the augmented prompt (as a system message) to the chat history so that the agent sees the retrieval context. The user message is also added, and then the agent is invoked using streaming. The output is printed as it streams in.

import asyncio
from IPython.display import display, HTML

async def main():
    thread: ChatHistoryAgentThread | None = None

    while True:
        user_input = await asyncio.to_thread(input, "\n🧠 You: ")

        if user_input.lower() in ["exit", "quit"]:
            print("👋 Exiting...")
            break

        html_output = (
            f"<div style='margin-bottom:10px'>"
            f"<div style='font-weight:bold'>User:</div>"
            f"<div style='margin-left:20px'>{user_input}</div></div>"
        )

        agent_name = None
        full_response: list[str] = []
        function_calls: list[str] = []

        current_function_name = None
        argument_buffer = ""

        async for response in agent.invoke_stream(
            messages=user_input,
            thread=thread,
        ):
            thread = response.thread
            agent_name = response.name
            content_items = list(response.items)

            for item in content_items:
                if isinstance(item, FunctionCallContent):
                    if item.function_name:
                        current_function_name = item.function_name
                    if isinstance(item.arguments, str):
                        argument_buffer += item.arguments
                elif isinstance(item, FunctionResultContent):
                    if current_function_name:
                        formatted_args = argument_buffer.strip()
                        try:
                            parsed_args = json.loads(formatted_args)
                            formatted_args = json.dumps(parsed_args)
                        except Exception:
                            pass

                        function_calls.append(f"Calling function: {current_function_name}({formatted_args})")
                        current_function_name = None
                        argument_buffer = ""

                    function_calls.append(f"\nFunction Result:\n\n{item.result}")
                elif isinstance(item, StreamingTextContent) and item.text:
                    full_response.append(item.text)

        if function_calls:
            html_output += (
                "<div style='margin-bottom:10px'>"
                "<details>"
                "<summary style='cursor:pointer; font-weight:bold; color:#0066cc;'>Function Calls (click to expand)</summary>"
                "<div style='margin:10px; padding:10px; background-color:#f8f8f8; "
                "border:1px solid #ddd; border-radius:4px; white-space:pre-wrap; font-size:14px; color:#333;'>"
                f"{chr(10).join(function_calls)}"
                "</div></details></div>"
            )

        html_output += (
            "<div style='margin-bottom:20px'>"
            f"<div style='font-weight:bold'>{agent_name or 'Assistant'}:</div>"
            f"<div style='margin-left:20px; white-space:pre-wrap'>{''.join(full_response)}</div></div><hr>"
        )

        display(HTML(html_output))

# Run it in an async environment
await main()
